{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d53da3f",
   "metadata": {},
   "source": [
    "## LLM Inference\n",
    "\n",
    "Cells below show how to use the OllamaClient and the HFClient to run basic inference. In general Ollama is faster, the ollama docker must be run concurrently. If the ollama docker doesn't work, its fine to use the HFClient. Recommended to use at minimum a 4B model for inference, since it needs to mimic a larger llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe37775a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derekvawdrey/Workspace/School/CS401R/CS401Rproject/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model qwen2.5:0.5b-instruct downloaded successfully.\n",
      "qwen2.5:0.5b-instruct\n",
      "Model qwen2.5:0.5b-instruct warmed up successfully.\n"
     ]
    }
   ],
   "source": [
    "from inference import OllamaClient\n",
    "client = OllamaClient(base_url=\"http://localhost:11434\")\n",
    "# model=\"gemma3:4b\"\n",
    "model=\"qwen2.5:0.5b-instruct\"\n",
    "client.download_model(model=model)\n",
    "# models = client.list_models()\n",
    "# print(models)\n",
    "print(model)\n",
    "client.warmup_model(model=model)\n",
    "# client.warmup_model(model=evaluator_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26514a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of cheese-making and related practices, \"cheese\" can refer to different things depending on the specific industry or culture in which it is used.\n",
      "\n",
      "For example:\n",
      "- In dairy farming and processing, cheese refers to a type of milk制成\n"
     ]
    }
   ],
   "source": [
    "prompt=\"What is the meaning of Cheese?\"\n",
    "res = client.generate(model=model, prompt=prompt, temperature=0.7, max_tokens=50)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7f02fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|██████████| 10/10 [00:36<00:00,  3.61s/it]\n"
     ]
    }
   ],
   "source": [
    "from inference import HFClient\n",
    "client = HFClient()\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# print(client.list_models())\n",
    "client.warmup_model(model_id=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "166407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is cheese? Cheese is a dairy product made from the curd or whey of milk. It is typically made by fermenting milk with bacteria, yeast, and sometimes other microorganisms to produce a semi-solid substance that can be shaped into various forms such as blocks\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is cheese?\"\n",
    "res = client.generate(model_id=model_id, prompt=prompt, max_new_tokens=50)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "941213e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: http://localhost:11434/api/generate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      4\u001b[39m dataset = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mopenai/gsm8k\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mPRewriteTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen2.5-0.5B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexact\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mollama\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/School/CS401R/CS401Rproject/finetune.py:181\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, model_id, dataset, evaluator_model, task, ollama)\u001b[39m\n\u001b[32m    179\u001b[39m tokenizer, model, ref_model = \u001b[38;5;28mself\u001b[39m.load_from_hf(model_id)\n\u001b[32m    180\u001b[39m device = model.device\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28mself\u001b[39m.reward_model = RewardModel(\u001b[38;5;28mself\u001b[39m.evaluator, data=dataset, device=device).to(device)\n\u001b[32m    182\u001b[39m ppo_config = PPOConfig(\n\u001b[32m    183\u001b[39m     exp_name=\u001b[33m\"\u001b[39m\u001b[33mprewrite_finetune\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    184\u001b[39m     num_ppo_epochs=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    189\u001b[39m     vf_coef=\u001b[32m0.1\u001b[39m,\n\u001b[32m    190\u001b[39m )\n\u001b[32m    191\u001b[39m optim = torch.optim.AdamW(\n\u001b[32m    192\u001b[39m     model.parameters(),\n\u001b[32m    193\u001b[39m     lr=\u001b[32m1e-5\u001b[39m\n\u001b[32m    194\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/School/CS401R/CS401Rproject/finetune.py:146\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, model, client, reward_mode)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mHFTaskEvaluator\u001b[39;00m(BaseTaskEvaluator):\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m, client: HFClient, reward_mode: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    147\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(reward_mode)\n\u001b[32m    148\u001b[39m         \u001b[38;5;28mself\u001b[39m.model=model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/School/CS401R/CS401Rproject/inference.py:119\u001b[39m, in \u001b[36mOllamaClient.warmup_model\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    115\u001b[39m payload = {\n\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m    117\u001b[39m }\n\u001b[32m    118\u001b[39m r = requests.post(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.base_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/generate\u001b[39m\u001b[33m\"\u001b[39m, json=payload, timeout=\u001b[38;5;28mself\u001b[39m.timeout)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m warmed up successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/School/CS401R/CS401Rproject/.venv/lib/python3.13/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: http://localhost:11434/api/generate"
     ]
    }
   ],
   "source": [
    "from finetune import PRewriteTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "PRewriteTrainer(\n",
    "    model_id=\"Qwen/Qwen2.5-0.5B-Instruct\", \n",
    "    dataset=dataset, \n",
    "    task=\"exact\", \n",
    "    ollama=True\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
